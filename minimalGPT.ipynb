{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPp0kR/prHH2tTBl0NUtAcB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AMX07/AWS_BDA/blob/main/minimalGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MV5Vmu0M0WvL"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os       # os.path.exists\n",
        "import math     # math.log, math.exp\n",
        "import random   # random.seed, random.choices, random.gauss, random.shuffle\n",
        "random.seed(42) # Let there be order among chaos\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be an input dataset `docs`: list[str] of documents (e.g. a dataset of names)\n",
        "if not os.path.exists('input.txt'):\n",
        "    import urllib.request\n",
        "    names_url = 'https://raw.githubusercontent.com/karpathy/makemore/refs/heads/master/names.txt'\n",
        "    urllib.request.urlretrieve(names_url, 'input.txt')\n",
        "docs = [l.strip() for l in open('input.txt').read().strip().split('\\n') if l.strip()] # list[str] of documents\n",
        "random.shuffle(docs)\n",
        "print(f\"num docs: {len(docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b4qbBmM0bc6",
        "outputId": "504e9bec-0861-4278-eb3b-45e481db6bd2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num docs: 32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be a Tokenizer to translate strings to discrete symbols and back\n",
        "uchars = sorted(set(''.join(docs))) # unique characters in the dataset become token ids 0..n-1\n",
        "BOS = len(uchars) # token id for the special Beginning of Sequence (BOS) token\n",
        "vocab_size = len(uchars) + 1 # total number of unique tokens, +1 is for BOS\n",
        "print(f\"vocab size: {vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yd-S8re30oiG",
        "outputId": "5b5fd1a0-8b51-47d3-c428-bd7d0aa4a5fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be Autograd, to recursively apply the chain rule through a computation graph\n",
        "class Value:\n",
        "    __slots__ = ('data', 'grad', '_children', '_local_grads') # Python optimization for memory usage\n",
        "\n",
        "    def __init__(self, data, children=(), local_grads=()):\n",
        "        self.data = data                # scalar value of this node calculated during forward pass\n",
        "        self.grad = 0                   # derivative of the loss w.r.t. this node, calculated in backward pass\n",
        "        self._children = children       # children of this node in the computation graph\n",
        "        self._local_grads = local_grads # local derivative of this node w.r.t. its children\n",
        "\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data + other.data, (self, other), (1, 1))\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        return Value(self.data * other.data, (self, other), (other.data, self.data))\n",
        "\n",
        "    def __pow__(self, other): return Value(self.data**other, (self,), (other * self.data**(other-1),))\n",
        "    def log(self): return Value(math.log(self.data), (self,), (1/self.data,))\n",
        "    def exp(self): return Value(math.exp(self.data), (self,), (math.exp(self.data),))\n",
        "    def relu(self): return Value(max(0, self.data), (self,), (float(self.data > 0),))\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._children:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.grad = 1\n",
        "        for v in reversed(topo):\n",
        "            for child, local_grad in zip(v._children, v._local_grads):\n",
        "                child.grad += local_grad * v.grad\n"
      ],
      "metadata": {
        "id": "5WwWwyI20qFx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the parameters, to store the knowledge of the model.\n",
        "n_embd = 16     # embedding dimension\n",
        "n_head = 4      # number of attention heads\n",
        "n_layer = 1     # number of layers\n",
        "block_size = 16 # maximum sequence length\n",
        "head_dim = n_embd // n_head # dimension of each head\n",
        "matrix = lambda nout, nin, std=0.08: [[Value(random.gauss(0, std)) for _ in range(nin)] for _ in range(nout)]\n",
        "state_dict = {'wte': matrix(vocab_size, n_embd), 'wpe': matrix(block_size, n_embd), 'lm_head': matrix(vocab_size, n_embd)}\n",
        "for i in range(n_layer):\n",
        "    state_dict[f'layer{i}.attn_wq'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wk'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wv'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.attn_wo'] = matrix(n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc1'] = matrix(4 * n_embd, n_embd)\n",
        "    state_dict[f'layer{i}.mlp_fc2'] = matrix(n_embd, 4 * n_embd)\n",
        "params = [p for mat in state_dict.values() for row in mat for p in row] # flatten params into a single list[Value]\n",
        "print(f\"num params: {len(params)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vQa8YP60uCw",
        "outputId": "1f02efe6-7b06-4cd5-9735-2d64fa7756ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num params: 4192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the model architecture: a stateless function mapping token sequence and parameters to logits over what comes next.\n",
        "# Follow GPT-2, blessed among the GPTs, with minor differences: layernorm -> rmsnorm, no biases, GeLU -> ReLU\n",
        "def linear(x, w):\n",
        "    return [sum(wi * xi for wi, xi in zip(wo, x)) for wo in w]\n",
        "\n",
        "def softmax(logits):\n",
        "    max_val = max(val.data for val in logits)\n",
        "    exps = [(val - max_val).exp() for val in logits]\n",
        "    total = sum(exps)\n",
        "    return [e / total for e in exps]\n",
        "\n",
        "def rmsnorm(x):\n",
        "    ms = sum(xi * xi for xi in x) / len(x)\n",
        "    scale = (ms + 1e-5) ** -0.5\n",
        "    return [xi * scale for xi in x]\n",
        "\n",
        "def gpt(token_id, pos_id, keys, values):\n",
        "    tok_emb = state_dict['wte'][token_id] # token embedding\n",
        "    pos_emb = state_dict['wpe'][pos_id] # position embedding\n",
        "    x = [t + p for t, p in zip(tok_emb, pos_emb)] # joint token and position embedding\n",
        "    x = rmsnorm(x)\n",
        "\n",
        "    for li in range(n_layer):\n",
        "        # 1) Multi-head attention block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        q = linear(x, state_dict[f'layer{li}.attn_wq'])\n",
        "        k = linear(x, state_dict[f'layer{li}.attn_wk'])\n",
        "        v = linear(x, state_dict[f'layer{li}.attn_wv'])\n",
        "        keys[li].append(k)\n",
        "        values[li].append(v)\n",
        "        x_attn = []\n",
        "        for h in range(n_head):\n",
        "            hs = h * head_dim\n",
        "            q_h = q[hs:hs+head_dim]\n",
        "            k_h = [ki[hs:hs+head_dim] for ki in keys[li]]\n",
        "            v_h = [vi[hs:hs+head_dim] for vi in values[li]]\n",
        "            attn_logits = [sum(q_h[j] * k_h[t][j] for j in range(head_dim)) / head_dim**0.5 for t in range(len(k_h))]\n",
        "            attn_weights = softmax(attn_logits)\n",
        "            head_out = [sum(attn_weights[t] * v_h[t][j] for t in range(len(v_h))) for j in range(head_dim)]\n",
        "            x_attn.extend(head_out)\n",
        "        x = linear(x_attn, state_dict[f'layer{li}.attn_wo'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "        # 2) MLP block\n",
        "        x_residual = x\n",
        "        x = rmsnorm(x)\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc1'])\n",
        "        x = [xi.relu() for xi in x]\n",
        "        x = linear(x, state_dict[f'layer{li}.mlp_fc2'])\n",
        "        x = [a + b for a, b in zip(x, x_residual)]\n",
        "\n",
        "    logits = linear(x, state_dict['lm_head'])\n",
        "    return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "JKOI_K3P0v4w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let there be Adam, the blessed optimizer and its buffers\n",
        "learning_rate, beta1, beta2, eps_adam = 0.01, 0.85, 0.99, 1e-8\n",
        "m = [0.0] * len(params) # first moment buffer\n",
        "v = [0.0] * len(params) # second moment buffer\n",
        "\n",
        "# Repeat in sequence\n",
        "num_steps = 1000 # number of training steps\n",
        "for step in range(num_steps):\n",
        "\n",
        "    # Take single document, tokenize it, surround it with BOS special token on both sides\n",
        "    doc = docs[step % len(docs)]\n",
        "    tokens = [BOS] + [uchars.index(ch) for ch in doc] + [BOS]\n",
        "    n = min(block_size, len(tokens) - 1)\n",
        "\n",
        "    # Forward the token sequence through the model, building up the computation graph all the way to the loss.\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    losses = []\n",
        "    for pos_id in range(n):\n",
        "        token_id, target_id = tokens[pos_id], tokens[pos_id + 1]\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax(logits)\n",
        "        loss_t = -probs[target_id].log()\n",
        "        losses.append(loss_t)\n",
        "    loss = (1 / n) * sum(losses) # final average loss over the document sequence. May yours be low.\n",
        "\n",
        "    # Backward the loss, calculating the gradients with respect to all model parameters.\n",
        "    loss.backward()\n",
        "\n",
        "    # Adam optimizer update: update the model parameters based on the corresponding gradients.\n",
        "    lr_t = learning_rate * (1 - step / num_steps) # linear learning rate decay\n",
        "    for i, p in enumerate(params):\n",
        "        m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
        "        v[i] = beta2 * v[i] + (1 - beta2) * p.grad ** 2\n",
        "        m_hat = m[i] / (1 - beta1 ** (step + 1))\n",
        "        v_hat = v[i] / (1 - beta2 ** (step + 1))\n",
        "        p.data -= lr_t * m_hat / (v_hat ** 0.5 + eps_adam)\n",
        "        p.grad = 0\n",
        "\n",
        "    print(f\"step {step+1:4d} / {num_steps:4d} | loss {loss.data:.4f}\")\n",
        "\n",
        "# Inference: may the model babble back to us\n",
        "temperature = 0.5 # in (0, 1], control the \"creativity\" of generated text, low to high\n",
        "print(\"\\n--- inference (new, hallucinated names) ---\")\n",
        "for sample_idx in range(20):\n",
        "    keys, values = [[] for _ in range(n_layer)], [[] for _ in range(n_layer)]\n",
        "    token_id = BOS\n",
        "    sample = []\n",
        "    for pos_id in range(block_size):\n",
        "        logits = gpt(token_id, pos_id, keys, values)\n",
        "        probs = softmax([l / temperature for l in logits])\n",
        "        token_id = random.choices(range(vocab_size), weights=[p.data for p in probs])[0]\n",
        "        if token_id == BOS:\n",
        "            break\n",
        "        sample.append(uchars[token_id])\n",
        "    print(f\"sample {sample_idx+1:2d}: {''.join(sample)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeEWqdgu0z39",
        "outputId": "2e2c8069-d2c9-4310-b938-c3a2330537d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    1 / 1000 | loss 3.3660\n",
            "step    2 / 1000 | loss 3.4243\n",
            "step    3 / 1000 | loss 3.1778\n",
            "step    4 / 1000 | loss 3.0664\n",
            "step    5 / 1000 | loss 3.2209\n",
            "step    6 / 1000 | loss 2.9452\n",
            "step    7 / 1000 | loss 3.2894\n",
            "step    8 / 1000 | loss 3.3245\n",
            "step    9 / 1000 | loss 2.8990\n",
            "step   10 / 1000 | loss 3.2229\n",
            "step   11 / 1000 | loss 2.7964\n",
            "step   12 / 1000 | loss 2.9345\n",
            "step   13 / 1000 | loss 3.0544\n",
            "step   14 / 1000 | loss 3.0905\n",
            "step   15 / 1000 | loss 3.0651\n",
            "step   16 / 1000 | loss 2.7337\n",
            "step   17 / 1000 | loss 2.8839\n",
            "step   18 / 1000 | loss 2.8977\n",
            "step   19 / 1000 | loss 2.7073\n",
            "step   20 / 1000 | loss 2.7453\n",
            "step   21 / 1000 | loss 3.7212\n",
            "step   22 / 1000 | loss 2.8026\n",
            "step   23 / 1000 | loss 2.8241\n",
            "step   24 / 1000 | loss 2.0374\n",
            "step   25 / 1000 | loss 3.3698\n",
            "step   26 / 1000 | loss 2.9154\n",
            "step   27 / 1000 | loss 3.2795\n",
            "step   28 / 1000 | loss 2.9195\n",
            "step   29 / 1000 | loss 2.3027\n",
            "step   30 / 1000 | loss 2.2691\n",
            "step   31 / 1000 | loss 2.8957\n",
            "step   32 / 1000 | loss 2.9539\n",
            "step   33 / 1000 | loss 2.6819\n",
            "step   34 / 1000 | loss 2.1899\n",
            "step   35 / 1000 | loss 3.1121\n",
            "step   36 / 1000 | loss 2.7269\n",
            "step   37 / 1000 | loss 2.4928\n",
            "step   38 / 1000 | loss 2.9746\n",
            "step   39 / 1000 | loss 2.2992\n",
            "step   40 / 1000 | loss 2.8604\n",
            "step   41 / 1000 | loss 2.3052\n",
            "step   42 / 1000 | loss 2.5615\n",
            "step   43 / 1000 | loss 2.9018\n",
            "step   44 / 1000 | loss 2.4472\n",
            "step   45 / 1000 | loss 2.1513\n",
            "step   46 / 1000 | loss 3.0613\n",
            "step   47 / 1000 | loss 2.5581\n",
            "step   48 / 1000 | loss 3.0171\n",
            "step   49 / 1000 | loss 2.6902\n",
            "step   50 / 1000 | loss 2.4050\n",
            "step   51 / 1000 | loss 3.6813\n",
            "step   52 / 1000 | loss 2.8990\n",
            "step   53 / 1000 | loss 3.0358\n",
            "step   54 / 1000 | loss 2.2217\n",
            "step   55 / 1000 | loss 2.7366\n",
            "step   56 / 1000 | loss 2.2113\n",
            "step   57 / 1000 | loss 2.6736\n",
            "step   58 / 1000 | loss 2.4947\n",
            "step   59 / 1000 | loss 2.6330\n",
            "step   60 / 1000 | loss 2.9024\n",
            "step   61 / 1000 | loss 2.6594\n",
            "step   62 / 1000 | loss 2.4527\n",
            "step   63 / 1000 | loss 2.7178\n",
            "step   64 / 1000 | loss 2.8619\n",
            "step   65 / 1000 | loss 2.8474\n",
            "step   66 / 1000 | loss 2.8673\n",
            "step   67 / 1000 | loss 2.7473\n",
            "step   68 / 1000 | loss 2.5459\n",
            "step   69 / 1000 | loss 2.5597\n",
            "step   70 / 1000 | loss 2.8365\n",
            "step   71 / 1000 | loss 3.4163\n",
            "step   72 / 1000 | loss 2.5205\n",
            "step   73 / 1000 | loss 2.5853\n",
            "step   74 / 1000 | loss 2.4236\n",
            "step   75 / 1000 | loss 2.4053\n",
            "step   76 / 1000 | loss 2.7836\n",
            "step   77 / 1000 | loss 2.8438\n",
            "step   78 / 1000 | loss 3.0302\n",
            "step   79 / 1000 | loss 2.3869\n",
            "step   80 / 1000 | loss 2.3910\n",
            "step   81 / 1000 | loss 2.3688\n",
            "step   82 / 1000 | loss 3.1079\n",
            "step   83 / 1000 | loss 2.5942\n",
            "step   84 / 1000 | loss 2.1857\n",
            "step   85 / 1000 | loss 2.1898\n",
            "step   86 / 1000 | loss 2.4624\n",
            "step   87 / 1000 | loss 2.9832\n",
            "step   88 / 1000 | loss 2.7304\n",
            "step   89 / 1000 | loss 2.6506\n",
            "step   90 / 1000 | loss 2.5744\n",
            "step   91 / 1000 | loss 2.5922\n",
            "step   92 / 1000 | loss 3.1646\n",
            "step   93 / 1000 | loss 2.9175\n",
            "step   94 / 1000 | loss 2.9945\n",
            "step   95 / 1000 | loss 2.3016\n",
            "step   96 / 1000 | loss 2.4319\n",
            "step   97 / 1000 | loss 2.0333\n",
            "step   98 / 1000 | loss 3.3962\n",
            "step   99 / 1000 | loss 2.2613\n",
            "step  100 / 1000 | loss 3.3669\n",
            "step  101 / 1000 | loss 2.4483\n",
            "step  102 / 1000 | loss 2.2889\n",
            "step  103 / 1000 | loss 2.7991\n",
            "step  104 / 1000 | loss 2.6872\n",
            "step  105 / 1000 | loss 2.6311\n",
            "step  106 / 1000 | loss 2.4243\n",
            "step  107 / 1000 | loss 2.8984\n",
            "step  108 / 1000 | loss 2.2613\n",
            "step  109 / 1000 | loss 2.2090\n",
            "step  110 / 1000 | loss 2.5113\n",
            "step  111 / 1000 | loss 2.6165\n",
            "step  112 / 1000 | loss 2.6483\n",
            "step  113 / 1000 | loss 2.6772\n",
            "step  114 / 1000 | loss 2.2588\n",
            "step  115 / 1000 | loss 2.2152\n",
            "step  116 / 1000 | loss 2.8152\n",
            "step  117 / 1000 | loss 2.6372\n",
            "step  118 / 1000 | loss 2.0875\n",
            "step  119 / 1000 | loss 2.5167\n",
            "step  120 / 1000 | loss 2.6920\n",
            "step  121 / 1000 | loss 2.3495\n",
            "step  122 / 1000 | loss 2.2998\n",
            "step  123 / 1000 | loss 2.7507\n",
            "step  124 / 1000 | loss 2.5124\n",
            "step  125 / 1000 | loss 3.0075\n",
            "step  126 / 1000 | loss 2.2402\n",
            "step  127 / 1000 | loss 2.6489\n",
            "step  128 / 1000 | loss 2.2248\n",
            "step  129 / 1000 | loss 2.1412\n",
            "step  130 / 1000 | loss 3.0851\n",
            "step  131 / 1000 | loss 2.8208\n",
            "step  132 / 1000 | loss 2.0810\n",
            "step  133 / 1000 | loss 2.8060\n",
            "step  134 / 1000 | loss 2.7096\n",
            "step  135 / 1000 | loss 2.6401\n",
            "step  136 / 1000 | loss 3.1040\n",
            "step  137 / 1000 | loss 2.1829\n",
            "step  138 / 1000 | loss 2.2031\n",
            "step  139 / 1000 | loss 2.7783\n",
            "step  140 / 1000 | loss 2.5121\n",
            "step  141 / 1000 | loss 3.2092\n",
            "step  142 / 1000 | loss 2.3187\n",
            "step  143 / 1000 | loss 3.1077\n",
            "step  144 / 1000 | loss 2.2708\n",
            "step  145 / 1000 | loss 2.6134\n",
            "step  146 / 1000 | loss 2.5703\n",
            "step  147 / 1000 | loss 2.2524\n",
            "step  148 / 1000 | loss 2.3684\n",
            "step  149 / 1000 | loss 2.1724\n",
            "step  150 / 1000 | loss 2.5351\n",
            "step  151 / 1000 | loss 2.9063\n",
            "step  152 / 1000 | loss 2.4862\n",
            "step  153 / 1000 | loss 2.8622\n",
            "step  154 / 1000 | loss 3.1861\n",
            "step  155 / 1000 | loss 2.5595\n",
            "step  156 / 1000 | loss 2.8595\n",
            "step  157 / 1000 | loss 3.2600\n",
            "step  158 / 1000 | loss 2.0037\n",
            "step  159 / 1000 | loss 3.3867\n",
            "step  160 / 1000 | loss 2.1104\n",
            "step  161 / 1000 | loss 2.2022\n",
            "step  162 / 1000 | loss 2.6278\n",
            "step  163 / 1000 | loss 2.5539\n",
            "step  164 / 1000 | loss 2.4285\n",
            "step  165 / 1000 | loss 2.3338\n",
            "step  166 / 1000 | loss 2.6923\n",
            "step  167 / 1000 | loss 2.1427\n",
            "step  168 / 1000 | loss 2.5276\n",
            "step  169 / 1000 | loss 3.1430\n",
            "step  170 / 1000 | loss 2.5338\n",
            "step  171 / 1000 | loss 2.6454\n",
            "step  172 / 1000 | loss 2.3900\n",
            "step  173 / 1000 | loss 2.2324\n",
            "step  174 / 1000 | loss 3.0033\n",
            "step  175 / 1000 | loss 2.6798\n",
            "step  176 / 1000 | loss 2.5880\n",
            "step  177 / 1000 | loss 3.1046\n",
            "step  178 / 1000 | loss 2.3841\n",
            "step  179 / 1000 | loss 1.9982\n",
            "step  180 / 1000 | loss 2.3556\n",
            "step  181 / 1000 | loss 2.1472\n",
            "step  182 / 1000 | loss 2.0537\n",
            "step  183 / 1000 | loss 1.9403\n",
            "step  184 / 1000 | loss 3.3390\n",
            "step  185 / 1000 | loss 2.1482\n",
            "step  186 / 1000 | loss 2.4919\n",
            "step  187 / 1000 | loss 2.4610\n",
            "step  188 / 1000 | loss 2.4055\n",
            "step  189 / 1000 | loss 1.9792\n",
            "step  190 / 1000 | loss 2.6377\n",
            "step  191 / 1000 | loss 1.7000\n",
            "step  192 / 1000 | loss 2.4035\n",
            "step  193 / 1000 | loss 2.2961\n",
            "step  194 / 1000 | loss 2.8886\n",
            "step  195 / 1000 | loss 2.8026\n",
            "step  196 / 1000 | loss 2.4264\n",
            "step  197 / 1000 | loss 2.3991\n",
            "step  198 / 1000 | loss 3.0697\n",
            "step  199 / 1000 | loss 2.5300\n",
            "step  200 / 1000 | loss 2.3097\n",
            "step  201 / 1000 | loss 2.4874\n",
            "step  202 / 1000 | loss 2.5649\n",
            "step  203 / 1000 | loss 2.1233\n",
            "step  204 / 1000 | loss 1.8898\n",
            "step  205 / 1000 | loss 2.6302\n",
            "step  206 / 1000 | loss 3.1559\n",
            "step  207 / 1000 | loss 2.8998\n",
            "step  208 / 1000 | loss 2.1443\n",
            "step  209 / 1000 | loss 2.2206\n",
            "step  210 / 1000 | loss 2.5670\n",
            "step  211 / 1000 | loss 2.0186\n",
            "step  212 / 1000 | loss 2.3012\n",
            "step  213 / 1000 | loss 3.8427\n",
            "step  214 / 1000 | loss 2.2129\n",
            "step  215 / 1000 | loss 2.4124\n",
            "step  216 / 1000 | loss 2.5136\n",
            "step  217 / 1000 | loss 2.3378\n",
            "step  218 / 1000 | loss 2.5365\n",
            "step  219 / 1000 | loss 2.3739\n",
            "step  220 / 1000 | loss 2.4205\n",
            "step  221 / 1000 | loss 3.0695\n",
            "step  222 / 1000 | loss 2.3135\n",
            "step  223 / 1000 | loss 2.1625\n",
            "step  224 / 1000 | loss 2.3273\n",
            "step  225 / 1000 | loss 2.2527\n",
            "step  226 / 1000 | loss 2.4193\n",
            "step  227 / 1000 | loss 2.4528\n",
            "step  228 / 1000 | loss 2.5524\n",
            "step  229 / 1000 | loss 3.0859\n",
            "step  230 / 1000 | loss 1.7900\n",
            "step  231 / 1000 | loss 3.1017\n",
            "step  232 / 1000 | loss 2.4001\n",
            "step  233 / 1000 | loss 2.3035\n",
            "step  234 / 1000 | loss 2.7662\n",
            "step  235 / 1000 | loss 2.0570\n",
            "step  236 / 1000 | loss 2.7383\n",
            "step  237 / 1000 | loss 2.2569\n",
            "step  238 / 1000 | loss 2.6960\n",
            "step  239 / 1000 | loss 2.4001\n",
            "step  240 / 1000 | loss 3.6365\n",
            "step  241 / 1000 | loss 2.8041\n",
            "step  242 / 1000 | loss 2.5392\n",
            "step  243 / 1000 | loss 2.3092\n",
            "step  244 / 1000 | loss 2.6435\n",
            "step  245 / 1000 | loss 2.2066\n",
            "step  246 / 1000 | loss 2.7219\n",
            "step  247 / 1000 | loss 2.4871\n",
            "step  248 / 1000 | loss 2.7047\n",
            "step  249 / 1000 | loss 2.0570\n",
            "step  250 / 1000 | loss 2.1581\n",
            "step  251 / 1000 | loss 1.9875\n",
            "step  252 / 1000 | loss 2.4351\n",
            "step  253 / 1000 | loss 2.7340\n",
            "step  254 / 1000 | loss 1.9832\n",
            "step  255 / 1000 | loss 2.4915\n",
            "step  256 / 1000 | loss 3.5044\n",
            "step  257 / 1000 | loss 2.3991\n",
            "step  258 / 1000 | loss 1.8618\n",
            "step  259 / 1000 | loss 1.9200\n",
            "step  260 / 1000 | loss 1.7671\n",
            "step  261 / 1000 | loss 2.6093\n",
            "step  262 / 1000 | loss 2.2438\n",
            "step  263 / 1000 | loss 2.9581\n",
            "step  264 / 1000 | loss 3.0106\n",
            "step  265 / 1000 | loss 1.8756\n",
            "step  266 / 1000 | loss 2.7724\n",
            "step  267 / 1000 | loss 1.9729\n",
            "step  268 / 1000 | loss 2.1480\n",
            "step  269 / 1000 | loss 2.1096\n",
            "step  270 / 1000 | loss 2.8207\n",
            "step  271 / 1000 | loss 2.2624\n",
            "step  272 / 1000 | loss 1.9211\n",
            "step  273 / 1000 | loss 2.6192\n",
            "step  274 / 1000 | loss 3.0047\n",
            "step  275 / 1000 | loss 2.0174\n",
            "step  276 / 1000 | loss 2.5915\n",
            "step  277 / 1000 | loss 3.1114\n",
            "step  278 / 1000 | loss 2.3490\n",
            "step  279 / 1000 | loss 2.3004\n",
            "step  280 / 1000 | loss 1.9486\n",
            "step  281 / 1000 | loss 3.1744\n",
            "step  282 / 1000 | loss 1.9351\n",
            "step  283 / 1000 | loss 2.4215\n",
            "step  284 / 1000 | loss 2.7351\n",
            "step  285 / 1000 | loss 3.3271\n",
            "step  286 / 1000 | loss 2.1280\n",
            "step  287 / 1000 | loss 2.3728\n",
            "step  288 / 1000 | loss 2.5311\n",
            "step  289 / 1000 | loss 2.4675\n",
            "step  290 / 1000 | loss 2.1163\n",
            "step  291 / 1000 | loss 3.0499\n",
            "step  292 / 1000 | loss 2.3976\n",
            "step  293 / 1000 | loss 1.9984\n",
            "step  294 / 1000 | loss 2.5432\n",
            "step  295 / 1000 | loss 2.7180\n",
            "step  296 / 1000 | loss 2.1555\n",
            "step  297 / 1000 | loss 2.3680\n",
            "step  298 / 1000 | loss 2.6502\n",
            "step  299 / 1000 | loss 2.1947\n",
            "step  300 / 1000 | loss 2.3178\n",
            "step  301 / 1000 | loss 2.6931\n",
            "step  302 / 1000 | loss 2.1736\n",
            "step  303 / 1000 | loss 2.6196\n",
            "step  304 / 1000 | loss 2.3674\n",
            "step  305 / 1000 | loss 2.8884\n",
            "step  306 / 1000 | loss 2.5560\n",
            "step  307 / 1000 | loss 1.9077\n",
            "step  308 / 1000 | loss 2.5663\n",
            "step  309 / 1000 | loss 2.0727\n",
            "step  310 / 1000 | loss 2.3818\n",
            "step  311 / 1000 | loss 3.0383\n",
            "step  312 / 1000 | loss 2.0074\n",
            "step  313 / 1000 | loss 1.7555\n",
            "step  314 / 1000 | loss 2.3456\n",
            "step  315 / 1000 | loss 2.6081\n",
            "step  316 / 1000 | loss 2.0439\n",
            "step  317 / 1000 | loss 2.0600\n",
            "step  318 / 1000 | loss 1.8657\n",
            "step  319 / 1000 | loss 1.9699\n",
            "step  320 / 1000 | loss 1.7969\n",
            "step  321 / 1000 | loss 2.5492\n",
            "step  322 / 1000 | loss 2.4804\n",
            "step  323 / 1000 | loss 2.7345\n",
            "step  324 / 1000 | loss 3.1487\n",
            "step  325 / 1000 | loss 2.4556\n",
            "step  326 / 1000 | loss 2.0597\n",
            "step  327 / 1000 | loss 2.3248\n",
            "step  328 / 1000 | loss 1.9027\n",
            "step  329 / 1000 | loss 2.1499\n",
            "step  330 / 1000 | loss 2.3627\n",
            "step  331 / 1000 | loss 2.1251\n",
            "step  332 / 1000 | loss 2.4151\n",
            "step  333 / 1000 | loss 1.9030\n",
            "step  334 / 1000 | loss 2.8525\n",
            "step  335 / 1000 | loss 3.9066\n",
            "step  336 / 1000 | loss 3.3516\n",
            "step  337 / 1000 | loss 2.6985\n",
            "step  338 / 1000 | loss 2.5921\n",
            "step  339 / 1000 | loss 2.2606\n",
            "step  340 / 1000 | loss 2.1589\n",
            "step  341 / 1000 | loss 3.0280\n",
            "step  342 / 1000 | loss 2.8235\n",
            "step  343 / 1000 | loss 1.8070\n",
            "step  344 / 1000 | loss 2.5350\n",
            "step  345 / 1000 | loss 2.4687\n",
            "step  346 / 1000 | loss 2.4156\n",
            "step  347 / 1000 | loss 2.9995\n",
            "step  348 / 1000 | loss 2.4981\n",
            "step  349 / 1000 | loss 2.6259\n",
            "step  350 / 1000 | loss 2.2592\n",
            "step  351 / 1000 | loss 3.1636\n",
            "step  352 / 1000 | loss 1.9862\n",
            "step  353 / 1000 | loss 2.3807\n",
            "step  354 / 1000 | loss 2.8480\n",
            "step  355 / 1000 | loss 2.5412\n",
            "step  356 / 1000 | loss 2.1290\n",
            "step  357 / 1000 | loss 2.7031\n",
            "step  358 / 1000 | loss 2.0749\n",
            "step  359 / 1000 | loss 1.9376\n",
            "step  360 / 1000 | loss 2.4932\n",
            "step  361 / 1000 | loss 2.5539\n",
            "step  362 / 1000 | loss 2.4702\n",
            "step  363 / 1000 | loss 1.8193\n",
            "step  364 / 1000 | loss 1.9877\n",
            "step  365 / 1000 | loss 2.6337\n",
            "step  366 / 1000 | loss 1.9184\n",
            "step  367 / 1000 | loss 3.0730\n",
            "step  368 / 1000 | loss 2.5106\n",
            "step  369 / 1000 | loss 2.8109\n",
            "step  370 / 1000 | loss 2.0459\n",
            "step  371 / 1000 | loss 2.9865\n",
            "step  372 / 1000 | loss 2.1616\n",
            "step  373 / 1000 | loss 2.7536\n",
            "step  374 / 1000 | loss 2.1206\n",
            "step  375 / 1000 | loss 1.9970\n",
            "step  376 / 1000 | loss 2.4778\n",
            "step  377 / 1000 | loss 2.3444\n",
            "step  378 / 1000 | loss 2.2609\n",
            "step  379 / 1000 | loss 2.4662\n",
            "step  380 / 1000 | loss 2.2087\n",
            "step  381 / 1000 | loss 2.4502\n",
            "step  382 / 1000 | loss 2.7536\n",
            "step  383 / 1000 | loss 2.3231\n",
            "step  384 / 1000 | loss 3.2495\n",
            "step  385 / 1000 | loss 2.9181\n",
            "step  386 / 1000 | loss 2.3336\n",
            "step  387 / 1000 | loss 3.6985\n",
            "step  388 / 1000 | loss 2.2499\n",
            "step  389 / 1000 | loss 2.3085\n",
            "step  390 / 1000 | loss 3.1236\n",
            "step  391 / 1000 | loss 2.4739\n",
            "step  392 / 1000 | loss 2.1051\n",
            "step  393 / 1000 | loss 2.1702\n",
            "step  394 / 1000 | loss 2.2743\n",
            "step  395 / 1000 | loss 2.6582\n",
            "step  396 / 1000 | loss 1.8241\n",
            "step  397 / 1000 | loss 2.0875\n",
            "step  398 / 1000 | loss 2.8767\n",
            "step  399 / 1000 | loss 2.7444\n",
            "step  400 / 1000 | loss 2.3428\n",
            "step  401 / 1000 | loss 2.6035\n",
            "step  402 / 1000 | loss 2.7292\n",
            "step  403 / 1000 | loss 1.9550\n",
            "step  404 / 1000 | loss 2.2429\n",
            "step  405 / 1000 | loss 2.7119\n",
            "step  406 / 1000 | loss 2.5498\n",
            "step  407 / 1000 | loss 2.2875\n",
            "step  408 / 1000 | loss 2.6208\n",
            "step  409 / 1000 | loss 2.8385\n",
            "step  410 / 1000 | loss 2.9415\n",
            "step  411 / 1000 | loss 2.2064\n",
            "step  412 / 1000 | loss 2.1636\n",
            "step  413 / 1000 | loss 2.2308\n",
            "step  414 / 1000 | loss 2.8363\n",
            "step  415 / 1000 | loss 2.0398\n",
            "step  416 / 1000 | loss 2.4377\n",
            "step  417 / 1000 | loss 2.9288\n",
            "step  418 / 1000 | loss 1.9164\n",
            "step  419 / 1000 | loss 2.4943\n",
            "step  420 / 1000 | loss 2.7135\n",
            "step  421 / 1000 | loss 2.5427\n",
            "step  422 / 1000 | loss 2.4804\n",
            "step  423 / 1000 | loss 1.9508\n",
            "step  424 / 1000 | loss 2.5618\n",
            "step  425 / 1000 | loss 2.6098\n",
            "step  426 / 1000 | loss 2.8338\n",
            "step  427 / 1000 | loss 2.4871\n",
            "step  428 / 1000 | loss 2.3602\n",
            "step  429 / 1000 | loss 2.0358\n",
            "step  430 / 1000 | loss 2.3998\n",
            "step  431 / 1000 | loss 2.1980\n",
            "step  432 / 1000 | loss 2.0428\n",
            "step  433 / 1000 | loss 2.3457\n",
            "step  434 / 1000 | loss 2.3509\n",
            "step  435 / 1000 | loss 2.4827\n",
            "step  436 / 1000 | loss 3.3131\n",
            "step  437 / 1000 | loss 2.7833\n",
            "step  438 / 1000 | loss 1.8821\n",
            "step  439 / 1000 | loss 1.9444\n",
            "step  440 / 1000 | loss 2.1377\n",
            "step  441 / 1000 | loss 2.6178\n",
            "step  442 / 1000 | loss 3.1046\n",
            "step  443 / 1000 | loss 3.2299\n",
            "step  444 / 1000 | loss 2.3159\n",
            "step  445 / 1000 | loss 2.3500\n",
            "step  446 / 1000 | loss 2.0550\n",
            "step  447 / 1000 | loss 2.0598\n",
            "step  448 / 1000 | loss 3.0721\n",
            "step  449 / 1000 | loss 2.1660\n",
            "step  450 / 1000 | loss 3.0903\n",
            "step  451 / 1000 | loss 2.8054\n",
            "step  452 / 1000 | loss 2.8289\n",
            "step  453 / 1000 | loss 2.5301\n",
            "step  454 / 1000 | loss 1.9576\n",
            "step  455 / 1000 | loss 2.2373\n",
            "step  456 / 1000 | loss 2.7489\n",
            "step  457 / 1000 | loss 2.5852\n",
            "step  458 / 1000 | loss 2.7530\n",
            "step  459 / 1000 | loss 1.7114\n",
            "step  460 / 1000 | loss 2.3958\n",
            "step  461 / 1000 | loss 1.8254\n",
            "step  462 / 1000 | loss 2.7834\n",
            "step  463 / 1000 | loss 2.0794\n",
            "step  464 / 1000 | loss 2.2029\n",
            "step  465 / 1000 | loss 2.7421\n",
            "step  466 / 1000 | loss 2.2871\n",
            "step  467 / 1000 | loss 2.2345\n",
            "step  468 / 1000 | loss 2.2340\n",
            "step  469 / 1000 | loss 2.3651\n",
            "step  470 / 1000 | loss 3.8820\n",
            "step  471 / 1000 | loss 2.5910\n",
            "step  472 / 1000 | loss 2.7750\n",
            "step  473 / 1000 | loss 2.6283\n",
            "step  474 / 1000 | loss 2.3571\n",
            "step  475 / 1000 | loss 2.4745\n",
            "step  476 / 1000 | loss 2.1805\n",
            "step  477 / 1000 | loss 2.9413\n",
            "step  478 / 1000 | loss 2.2745\n",
            "step  479 / 1000 | loss 2.0886\n",
            "step  480 / 1000 | loss 2.0120\n",
            "step  481 / 1000 | loss 2.9853\n",
            "step  482 / 1000 | loss 2.7189\n",
            "step  483 / 1000 | loss 2.3466\n",
            "step  484 / 1000 | loss 2.8693\n",
            "step  485 / 1000 | loss 2.4805\n",
            "step  486 / 1000 | loss 2.1715\n",
            "step  487 / 1000 | loss 2.7516\n",
            "step  488 / 1000 | loss 2.6655\n",
            "step  489 / 1000 | loss 2.3425\n",
            "step  490 / 1000 | loss 2.2978\n",
            "step  491 / 1000 | loss 2.2573\n",
            "step  492 / 1000 | loss 2.3424\n",
            "step  493 / 1000 | loss 2.4360\n",
            "step  494 / 1000 | loss 2.1313\n",
            "step  495 / 1000 | loss 2.4870\n",
            "step  496 / 1000 | loss 2.5856\n",
            "step  497 / 1000 | loss 2.9952\n",
            "step  498 / 1000 | loss 2.4689\n",
            "step  499 / 1000 | loss 2.2353\n",
            "step  500 / 1000 | loss 2.0645\n",
            "step  501 / 1000 | loss 2.4261\n",
            "step  502 / 1000 | loss 2.1254\n",
            "step  503 / 1000 | loss 2.7352\n",
            "step  504 / 1000 | loss 2.0662\n",
            "step  505 / 1000 | loss 2.3327\n",
            "step  506 / 1000 | loss 2.4337\n",
            "step  507 / 1000 | loss 2.4315\n",
            "step  508 / 1000 | loss 2.6284\n",
            "step  509 / 1000 | loss 2.8761\n",
            "step  510 / 1000 | loss 2.7854\n",
            "step  511 / 1000 | loss 2.2922\n",
            "step  512 / 1000 | loss 2.2573\n",
            "step  513 / 1000 | loss 2.4115\n",
            "step  514 / 1000 | loss 2.8810\n",
            "step  515 / 1000 | loss 2.6771\n",
            "step  516 / 1000 | loss 3.0052\n",
            "step  517 / 1000 | loss 2.1366\n",
            "step  518 / 1000 | loss 2.2575\n",
            "step  519 / 1000 | loss 2.0644\n",
            "step  520 / 1000 | loss 2.7970\n",
            "step  521 / 1000 | loss 1.6685\n",
            "step  522 / 1000 | loss 1.8816\n",
            "step  523 / 1000 | loss 2.1512\n",
            "step  524 / 1000 | loss 2.4364\n",
            "step  525 / 1000 | loss 2.3002\n",
            "step  526 / 1000 | loss 2.6904\n",
            "step  527 / 1000 | loss 1.7979\n",
            "step  528 / 1000 | loss 2.5294\n",
            "step  529 / 1000 | loss 2.3032\n",
            "step  530 / 1000 | loss 1.6063\n",
            "step  531 / 1000 | loss 2.5921\n",
            "step  532 / 1000 | loss 2.3464\n",
            "step  533 / 1000 | loss 3.5815\n",
            "step  534 / 1000 | loss 2.2109\n",
            "step  535 / 1000 | loss 3.1679\n",
            "step  536 / 1000 | loss 1.8492\n",
            "step  537 / 1000 | loss 1.5782\n",
            "step  538 / 1000 | loss 2.4474\n",
            "step  539 / 1000 | loss 1.8286\n",
            "step  540 / 1000 | loss 2.7201\n",
            "step  541 / 1000 | loss 2.7791\n",
            "step  542 / 1000 | loss 1.9045\n",
            "step  543 / 1000 | loss 3.2878\n",
            "step  544 / 1000 | loss 2.3980\n",
            "step  545 / 1000 | loss 2.8266\n",
            "step  546 / 1000 | loss 2.4227\n",
            "step  547 / 1000 | loss 2.1204\n",
            "step  548 / 1000 | loss 2.8575\n",
            "step  549 / 1000 | loss 2.0631\n",
            "step  550 / 1000 | loss 1.9310\n",
            "step  551 / 1000 | loss 2.6828\n",
            "step  552 / 1000 | loss 2.4919\n",
            "step  553 / 1000 | loss 2.5412\n",
            "step  554 / 1000 | loss 2.7195\n",
            "step  555 / 1000 | loss 2.9065\n",
            "step  556 / 1000 | loss 2.3740\n",
            "step  557 / 1000 | loss 2.5296\n",
            "step  558 / 1000 | loss 1.9853\n",
            "step  559 / 1000 | loss 2.5890\n",
            "step  560 / 1000 | loss 3.1969\n",
            "step  561 / 1000 | loss 1.8082\n",
            "step  562 / 1000 | loss 2.9966\n",
            "step  563 / 1000 | loss 2.3597\n",
            "step  564 / 1000 | loss 2.0989\n",
            "step  565 / 1000 | loss 3.0321\n",
            "step  566 / 1000 | loss 1.7108\n",
            "step  567 / 1000 | loss 2.5155\n",
            "step  568 / 1000 | loss 2.7469\n",
            "step  569 / 1000 | loss 2.5179\n",
            "step  570 / 1000 | loss 2.8211\n",
            "step  571 / 1000 | loss 1.9473\n",
            "step  572 / 1000 | loss 3.1226\n",
            "step  573 / 1000 | loss 3.0085\n",
            "step  574 / 1000 | loss 2.4998\n",
            "step  575 / 1000 | loss 2.2788\n",
            "step  576 / 1000 | loss 2.0708\n",
            "step  577 / 1000 | loss 1.9976\n",
            "step  578 / 1000 | loss 2.6646\n",
            "step  579 / 1000 | loss 2.0727\n",
            "step  580 / 1000 | loss 2.4337\n",
            "step  581 / 1000 | loss 2.3260\n",
            "step  582 / 1000 | loss 2.4394\n",
            "step  583 / 1000 | loss 2.7697\n",
            "step  584 / 1000 | loss 2.9226\n",
            "step  585 / 1000 | loss 2.2863\n",
            "step  586 / 1000 | loss 2.4610\n",
            "step  587 / 1000 | loss 2.1763\n",
            "step  588 / 1000 | loss 2.0816\n",
            "step  589 / 1000 | loss 1.8776\n",
            "step  590 / 1000 | loss 2.4569\n",
            "step  591 / 1000 | loss 2.4763\n",
            "step  592 / 1000 | loss 2.1966\n",
            "step  593 / 1000 | loss 2.3452\n",
            "step  594 / 1000 | loss 2.8074\n",
            "step  595 / 1000 | loss 2.4341\n",
            "step  596 / 1000 | loss 2.1553\n",
            "step  597 / 1000 | loss 2.6157\n",
            "step  598 / 1000 | loss 2.1024\n",
            "step  599 / 1000 | loss 2.3983\n",
            "step  600 / 1000 | loss 2.4851\n",
            "step  601 / 1000 | loss 2.1083\n",
            "step  602 / 1000 | loss 2.4919\n",
            "step  603 / 1000 | loss 2.7452\n",
            "step  604 / 1000 | loss 2.0589\n",
            "step  605 / 1000 | loss 2.7860\n",
            "step  606 / 1000 | loss 1.7675\n",
            "step  607 / 1000 | loss 2.7445\n",
            "step  608 / 1000 | loss 2.2072\n",
            "step  609 / 1000 | loss 2.3056\n",
            "step  610 / 1000 | loss 2.4470\n",
            "step  611 / 1000 | loss 2.6861\n",
            "step  612 / 1000 | loss 2.5383\n",
            "step  613 / 1000 | loss 1.9791\n",
            "step  614 / 1000 | loss 2.1122\n",
            "step  615 / 1000 | loss 2.4416\n",
            "step  616 / 1000 | loss 2.9865\n",
            "step  617 / 1000 | loss 2.7236\n",
            "step  618 / 1000 | loss 2.3293\n",
            "step  619 / 1000 | loss 2.4571\n",
            "step  620 / 1000 | loss 2.6560\n",
            "step  621 / 1000 | loss 1.8379\n",
            "step  622 / 1000 | loss 2.2556\n",
            "step  623 / 1000 | loss 2.0642\n",
            "step  624 / 1000 | loss 2.4819\n",
            "step  625 / 1000 | loss 1.7747\n",
            "step  626 / 1000 | loss 2.5039\n",
            "step  627 / 1000 | loss 2.0995\n",
            "step  628 / 1000 | loss 2.2031\n",
            "step  629 / 1000 | loss 2.6526\n",
            "step  630 / 1000 | loss 2.6197\n",
            "step  631 / 1000 | loss 3.0481\n",
            "step  632 / 1000 | loss 1.7443\n",
            "step  633 / 1000 | loss 2.6695\n",
            "step  634 / 1000 | loss 2.5338\n",
            "step  635 / 1000 | loss 3.2450\n",
            "step  636 / 1000 | loss 2.8575\n",
            "step  637 / 1000 | loss 2.5257\n",
            "step  638 / 1000 | loss 2.2855\n",
            "step  639 / 1000 | loss 2.6202\n",
            "step  640 / 1000 | loss 1.9703\n",
            "step  641 / 1000 | loss 2.2895\n",
            "step  642 / 1000 | loss 1.9095\n",
            "step  643 / 1000 | loss 2.5737\n",
            "step  644 / 1000 | loss 2.2433\n",
            "step  645 / 1000 | loss 2.3000\n",
            "step  646 / 1000 | loss 2.0239\n",
            "step  647 / 1000 | loss 2.3138\n",
            "step  648 / 1000 | loss 3.1185\n",
            "step  649 / 1000 | loss 2.1672\n",
            "step  650 / 1000 | loss 2.6138\n",
            "step  651 / 1000 | loss 2.4730\n",
            "step  652 / 1000 | loss 2.4868\n",
            "step  653 / 1000 | loss 2.3750\n",
            "step  654 / 1000 | loss 2.1639\n",
            "step  655 / 1000 | loss 3.0494\n",
            "step  656 / 1000 | loss 2.4772\n",
            "step  657 / 1000 | loss 2.1428\n",
            "step  658 / 1000 | loss 2.9535\n",
            "step  659 / 1000 | loss 2.5928\n",
            "step  660 / 1000 | loss 2.4115\n",
            "step  661 / 1000 | loss 2.1242\n",
            "step  662 / 1000 | loss 2.9471\n",
            "step  663 / 1000 | loss 2.6772\n",
            "step  664 / 1000 | loss 2.6958\n",
            "step  665 / 1000 | loss 2.4493\n",
            "step  666 / 1000 | loss 2.0646\n",
            "step  667 / 1000 | loss 2.9612\n",
            "step  668 / 1000 | loss 2.8441\n",
            "step  669 / 1000 | loss 2.1719\n",
            "step  670 / 1000 | loss 2.1952\n",
            "step  671 / 1000 | loss 2.1350\n",
            "step  672 / 1000 | loss 1.8856\n",
            "step  673 / 1000 | loss 2.5404\n",
            "step  674 / 1000 | loss 2.4887\n",
            "step  675 / 1000 | loss 2.7627\n",
            "step  676 / 1000 | loss 2.1296\n",
            "step  677 / 1000 | loss 2.0944\n",
            "step  678 / 1000 | loss 2.2733\n",
            "step  679 / 1000 | loss 2.3283\n",
            "step  680 / 1000 | loss 2.2191\n",
            "step  681 / 1000 | loss 2.9738\n",
            "step  682 / 1000 | loss 2.0353\n",
            "step  683 / 1000 | loss 1.5894\n",
            "step  684 / 1000 | loss 2.3880\n",
            "step  685 / 1000 | loss 1.8963\n",
            "step  686 / 1000 | loss 2.4264\n",
            "step  687 / 1000 | loss 1.8933\n",
            "step  688 / 1000 | loss 2.3557\n",
            "step  689 / 1000 | loss 2.3917\n",
            "step  690 / 1000 | loss 2.3202\n",
            "step  691 / 1000 | loss 2.0521\n",
            "step  692 / 1000 | loss 1.8742\n",
            "step  693 / 1000 | loss 2.1245\n",
            "step  694 / 1000 | loss 3.7008\n",
            "step  695 / 1000 | loss 2.7782\n",
            "step  696 / 1000 | loss 2.4651\n",
            "step  697 / 1000 | loss 3.2385\n",
            "step  698 / 1000 | loss 2.6590\n",
            "step  699 / 1000 | loss 2.6012\n",
            "step  700 / 1000 | loss 2.3357\n",
            "step  701 / 1000 | loss 2.1908\n",
            "step  702 / 1000 | loss 3.2303\n",
            "step  703 / 1000 | loss 2.5401\n",
            "step  704 / 1000 | loss 2.0141\n",
            "step  705 / 1000 | loss 2.2466\n",
            "step  706 / 1000 | loss 2.2559\n",
            "step  707 / 1000 | loss 2.6487\n",
            "step  708 / 1000 | loss 2.7316\n",
            "step  709 / 1000 | loss 2.0201\n",
            "step  710 / 1000 | loss 2.2398\n",
            "step  711 / 1000 | loss 2.8304\n",
            "step  712 / 1000 | loss 2.4438\n",
            "step  713 / 1000 | loss 2.4199\n",
            "step  714 / 1000 | loss 2.5542\n",
            "step  715 / 1000 | loss 1.9634\n",
            "step  716 / 1000 | loss 1.8876\n",
            "step  717 / 1000 | loss 2.1661\n",
            "step  718 / 1000 | loss 2.0400\n",
            "step  719 / 1000 | loss 2.6692\n",
            "step  720 / 1000 | loss 2.1266\n",
            "step  721 / 1000 | loss 2.1274\n",
            "step  722 / 1000 | loss 2.6668\n",
            "step  723 / 1000 | loss 2.1620\n",
            "step  724 / 1000 | loss 2.7405\n",
            "step  725 / 1000 | loss 2.8878\n",
            "step  726 / 1000 | loss 2.6247\n",
            "step  727 / 1000 | loss 1.7349\n",
            "step  728 / 1000 | loss 2.1850\n",
            "step  729 / 1000 | loss 2.2787\n",
            "step  730 / 1000 | loss 2.2568\n",
            "step  731 / 1000 | loss 2.5408\n",
            "step  732 / 1000 | loss 2.5605\n",
            "step  733 / 1000 | loss 2.5687\n",
            "step  734 / 1000 | loss 2.9981\n",
            "step  735 / 1000 | loss 3.1957\n",
            "step  736 / 1000 | loss 2.4961\n",
            "step  737 / 1000 | loss 3.1245\n",
            "step  738 / 1000 | loss 1.8570\n",
            "step  739 / 1000 | loss 2.1931\n",
            "step  740 / 1000 | loss 3.2648\n",
            "step  741 / 1000 | loss 2.7264\n",
            "step  742 / 1000 | loss 2.7551\n",
            "step  743 / 1000 | loss 2.4624\n",
            "step  744 / 1000 | loss 2.4762\n",
            "step  745 / 1000 | loss 2.1545\n",
            "step  746 / 1000 | loss 2.8443\n",
            "step  747 / 1000 | loss 2.7363\n",
            "step  748 / 1000 | loss 2.8508\n",
            "step  749 / 1000 | loss 2.4379\n",
            "step  750 / 1000 | loss 2.0780\n",
            "step  751 / 1000 | loss 2.3346\n",
            "step  752 / 1000 | loss 1.8021\n",
            "step  753 / 1000 | loss 3.0455\n",
            "step  754 / 1000 | loss 2.4193\n",
            "step  755 / 1000 | loss 2.6941\n",
            "step  756 / 1000 | loss 2.6088\n",
            "step  757 / 1000 | loss 2.4175\n",
            "step  758 / 1000 | loss 2.3642\n",
            "step  759 / 1000 | loss 2.2976\n",
            "step  760 / 1000 | loss 2.6314\n",
            "step  761 / 1000 | loss 2.4459\n",
            "step  762 / 1000 | loss 2.1060\n",
            "step  763 / 1000 | loss 2.1103\n",
            "step  764 / 1000 | loss 2.0754\n",
            "step  765 / 1000 | loss 2.0023\n",
            "step  766 / 1000 | loss 2.4650\n",
            "step  767 / 1000 | loss 2.7972\n",
            "step  768 / 1000 | loss 2.5172\n",
            "step  769 / 1000 | loss 1.9687\n",
            "step  770 / 1000 | loss 2.2378\n",
            "step  771 / 1000 | loss 2.4757\n",
            "step  772 / 1000 | loss 2.0182\n",
            "step  773 / 1000 | loss 1.7604\n",
            "step  774 / 1000 | loss 2.6085\n",
            "step  775 / 1000 | loss 2.8517\n",
            "step  776 / 1000 | loss 1.8031\n",
            "step  777 / 1000 | loss 2.8917\n",
            "step  778 / 1000 | loss 2.0491\n",
            "step  779 / 1000 | loss 2.7976\n",
            "step  780 / 1000 | loss 2.0455\n",
            "step  781 / 1000 | loss 1.8593\n",
            "step  782 / 1000 | loss 2.8120\n",
            "step  783 / 1000 | loss 1.7626\n",
            "step  784 / 1000 | loss 2.4415\n",
            "step  785 / 1000 | loss 2.2726\n",
            "step  786 / 1000 | loss 2.1308\n",
            "step  787 / 1000 | loss 2.6911\n",
            "step  788 / 1000 | loss 2.7761\n",
            "step  789 / 1000 | loss 1.9803\n",
            "step  790 / 1000 | loss 2.2568\n",
            "step  791 / 1000 | loss 2.1672\n",
            "step  792 / 1000 | loss 2.1728\n",
            "step  793 / 1000 | loss 2.1669\n",
            "step  794 / 1000 | loss 2.2664\n",
            "step  795 / 1000 | loss 1.9224\n",
            "step  796 / 1000 | loss 2.4312\n",
            "step  797 / 1000 | loss 2.1678\n",
            "step  798 / 1000 | loss 2.7734\n",
            "step  799 / 1000 | loss 1.8007\n",
            "step  800 / 1000 | loss 2.2632\n",
            "step  801 / 1000 | loss 2.1064\n",
            "step  802 / 1000 | loss 2.1354\n",
            "step  803 / 1000 | loss 1.8741\n",
            "step  804 / 1000 | loss 1.9609\n",
            "step  805 / 1000 | loss 2.6330\n",
            "step  806 / 1000 | loss 2.1938\n",
            "step  807 / 1000 | loss 2.1032\n",
            "step  808 / 1000 | loss 1.9303\n",
            "step  809 / 1000 | loss 2.2945\n",
            "step  810 / 1000 | loss 2.0318\n",
            "step  811 / 1000 | loss 2.0004\n",
            "step  812 / 1000 | loss 2.3280\n",
            "step  813 / 1000 | loss 2.4433\n",
            "step  814 / 1000 | loss 2.2201\n",
            "step  815 / 1000 | loss 2.2991\n",
            "step  816 / 1000 | loss 2.7418\n",
            "step  817 / 1000 | loss 1.7048\n",
            "step  818 / 1000 | loss 2.5284\n",
            "step  819 / 1000 | loss 1.9636\n",
            "step  820 / 1000 | loss 2.4420\n",
            "step  821 / 1000 | loss 2.2038\n",
            "step  822 / 1000 | loss 2.5172\n",
            "step  823 / 1000 | loss 1.7077\n",
            "step  824 / 1000 | loss 2.2398\n",
            "step  825 / 1000 | loss 2.8151\n",
            "step  826 / 1000 | loss 2.4977\n",
            "step  827 / 1000 | loss 2.6141\n",
            "step  828 / 1000 | loss 2.7821\n",
            "step  829 / 1000 | loss 1.8019\n",
            "step  830 / 1000 | loss 2.4835\n",
            "step  831 / 1000 | loss 2.0712\n",
            "step  832 / 1000 | loss 2.0766\n",
            "step  833 / 1000 | loss 1.8469\n",
            "step  834 / 1000 | loss 2.2951\n",
            "step  835 / 1000 | loss 2.4414\n",
            "step  836 / 1000 | loss 2.5103\n",
            "step  837 / 1000 | loss 3.3694\n",
            "step  838 / 1000 | loss 2.3500\n",
            "step  839 / 1000 | loss 2.3950\n",
            "step  840 / 1000 | loss 2.5399\n",
            "step  841 / 1000 | loss 2.9150\n",
            "step  842 / 1000 | loss 2.4967\n",
            "step  843 / 1000 | loss 1.9816\n",
            "step  844 / 1000 | loss 2.6846\n",
            "step  845 / 1000 | loss 2.5020\n",
            "step  846 / 1000 | loss 1.8127\n",
            "step  847 / 1000 | loss 2.8528\n",
            "step  848 / 1000 | loss 2.0746\n",
            "step  849 / 1000 | loss 1.5794\n",
            "step  850 / 1000 | loss 2.4860\n",
            "step  851 / 1000 | loss 2.7039\n",
            "step  852 / 1000 | loss 2.1478\n",
            "step  853 / 1000 | loss 2.3845\n",
            "step  854 / 1000 | loss 2.3782\n",
            "step  855 / 1000 | loss 2.3659\n",
            "step  856 / 1000 | loss 2.1089\n",
            "step  857 / 1000 | loss 2.8112\n",
            "step  858 / 1000 | loss 2.7589\n",
            "step  859 / 1000 | loss 2.1425\n",
            "step  860 / 1000 | loss 2.4466\n",
            "step  861 / 1000 | loss 2.6435\n",
            "step  862 / 1000 | loss 2.6565\n",
            "step  863 / 1000 | loss 2.5271\n",
            "step  864 / 1000 | loss 3.1404\n",
            "step  865 / 1000 | loss 2.0112\n",
            "step  866 / 1000 | loss 2.0564\n",
            "step  867 / 1000 | loss 2.1266\n",
            "step  868 / 1000 | loss 1.8993\n",
            "step  869 / 1000 | loss 2.4955\n",
            "step  870 / 1000 | loss 2.7364\n",
            "step  871 / 1000 | loss 2.2273\n",
            "step  872 / 1000 | loss 2.3312\n",
            "step  873 / 1000 | loss 2.7687\n",
            "step  874 / 1000 | loss 2.2820\n",
            "step  875 / 1000 | loss 2.2595\n",
            "step  876 / 1000 | loss 2.3459\n",
            "step  877 / 1000 | loss 2.0663\n",
            "step  878 / 1000 | loss 2.7865\n",
            "step  879 / 1000 | loss 2.1826\n",
            "step  880 / 1000 | loss 2.6298\n",
            "step  881 / 1000 | loss 2.3814\n",
            "step  882 / 1000 | loss 1.8578\n",
            "step  883 / 1000 | loss 2.1931\n",
            "step  884 / 1000 | loss 2.1980\n",
            "step  885 / 1000 | loss 2.2070\n",
            "step  886 / 1000 | loss 2.1261\n",
            "step  887 / 1000 | loss 3.0004\n",
            "step  888 / 1000 | loss 2.2790\n",
            "step  889 / 1000 | loss 2.6385\n",
            "step  890 / 1000 | loss 2.0798\n",
            "step  891 / 1000 | loss 2.1188\n",
            "step  892 / 1000 | loss 3.4579\n",
            "step  893 / 1000 | loss 2.0826\n",
            "step  894 / 1000 | loss 1.7378\n",
            "step  895 / 1000 | loss 2.0197\n",
            "step  896 / 1000 | loss 2.4508\n",
            "step  897 / 1000 | loss 2.2737\n",
            "step  898 / 1000 | loss 1.9217\n",
            "step  899 / 1000 | loss 2.2933\n",
            "step  900 / 1000 | loss 2.7785\n",
            "step  901 / 1000 | loss 2.0881\n",
            "step  902 / 1000 | loss 2.3490\n",
            "step  903 / 1000 | loss 1.7459\n",
            "step  904 / 1000 | loss 2.0612\n",
            "step  905 / 1000 | loss 2.1511\n",
            "step  906 / 1000 | loss 1.9278\n",
            "step  907 / 1000 | loss 2.6180\n",
            "step  908 / 1000 | loss 2.3714\n",
            "step  909 / 1000 | loss 2.2607\n",
            "step  910 / 1000 | loss 2.7556\n",
            "step  911 / 1000 | loss 2.2940\n",
            "step  912 / 1000 | loss 2.6726\n",
            "step  913 / 1000 | loss 2.4291\n",
            "step  914 / 1000 | loss 2.8404\n",
            "step  915 / 1000 | loss 2.2663\n",
            "step  916 / 1000 | loss 2.3037\n",
            "step  917 / 1000 | loss 2.2782\n",
            "step  918 / 1000 | loss 2.4194\n",
            "step  919 / 1000 | loss 2.4164\n",
            "step  920 / 1000 | loss 2.6305\n",
            "step  921 / 1000 | loss 1.9157\n",
            "step  922 / 1000 | loss 1.8924\n",
            "step  923 / 1000 | loss 2.0604\n",
            "step  924 / 1000 | loss 2.5970\n",
            "step  925 / 1000 | loss 2.1268\n",
            "step  926 / 1000 | loss 2.0386\n",
            "step  927 / 1000 | loss 2.5987\n",
            "step  928 / 1000 | loss 2.3180\n",
            "step  929 / 1000 | loss 1.8104\n",
            "step  930 / 1000 | loss 2.4971\n",
            "step  931 / 1000 | loss 3.1351\n",
            "step  932 / 1000 | loss 2.3636\n",
            "step  933 / 1000 | loss 2.4958\n",
            "step  934 / 1000 | loss 2.1538\n",
            "step  935 / 1000 | loss 2.0586\n",
            "step  936 / 1000 | loss 1.8687\n",
            "step  937 / 1000 | loss 1.8116\n",
            "step  938 / 1000 | loss 1.6251\n",
            "step  939 / 1000 | loss 1.9955\n",
            "step  940 / 1000 | loss 1.7995\n",
            "step  941 / 1000 | loss 1.9697\n",
            "step  942 / 1000 | loss 2.1796\n",
            "step  943 / 1000 | loss 1.9453\n",
            "step  944 / 1000 | loss 2.6730\n",
            "step  945 / 1000 | loss 2.1508\n",
            "step  946 / 1000 | loss 2.3271\n",
            "step  947 / 1000 | loss 2.0929\n",
            "step  948 / 1000 | loss 1.7849\n",
            "step  949 / 1000 | loss 1.9801\n",
            "step  950 / 1000 | loss 2.3016\n",
            "step  951 / 1000 | loss 2.7790\n",
            "step  952 / 1000 | loss 2.0783\n",
            "step  953 / 1000 | loss 2.2319\n",
            "step  954 / 1000 | loss 2.1295\n",
            "step  955 / 1000 | loss 2.5928\n",
            "step  956 / 1000 | loss 3.0061\n",
            "step  957 / 1000 | loss 2.1160\n",
            "step  958 / 1000 | loss 2.2593\n",
            "step  959 / 1000 | loss 2.0209\n",
            "step  960 / 1000 | loss 2.1214\n",
            "step  961 / 1000 | loss 2.2633\n",
            "step  962 / 1000 | loss 2.3385\n",
            "step  963 / 1000 | loss 2.5537\n",
            "step  964 / 1000 | loss 2.7235\n",
            "step  965 / 1000 | loss 3.3042\n",
            "step  966 / 1000 | loss 2.1621\n",
            "step  967 / 1000 | loss 2.9326\n",
            "step  968 / 1000 | loss 1.8063\n",
            "step  969 / 1000 | loss 2.2380\n",
            "step  970 / 1000 | loss 1.9579\n",
            "step  971 / 1000 | loss 2.3572\n",
            "step  972 / 1000 | loss 2.1710\n",
            "step  973 / 1000 | loss 2.5142\n",
            "step  974 / 1000 | loss 2.0779\n",
            "step  975 / 1000 | loss 1.9271\n",
            "step  976 / 1000 | loss 2.0277\n",
            "step  977 / 1000 | loss 2.5328\n",
            "step  978 / 1000 | loss 1.8817\n",
            "step  979 / 1000 | loss 1.9636\n",
            "step  980 / 1000 | loss 1.9525\n",
            "step  981 / 1000 | loss 2.4269\n",
            "step  982 / 1000 | loss 2.8226\n",
            "step  983 / 1000 | loss 2.4713\n",
            "step  984 / 1000 | loss 2.0303\n",
            "step  985 / 1000 | loss 2.7422\n",
            "step  986 / 1000 | loss 2.6811\n",
            "step  987 / 1000 | loss 1.9173\n",
            "step  988 / 1000 | loss 2.4303\n",
            "step  989 / 1000 | loss 2.4466\n",
            "step  990 / 1000 | loss 2.6354\n",
            "step  991 / 1000 | loss 2.1729\n",
            "step  992 / 1000 | loss 1.9659\n",
            "step  993 / 1000 | loss 2.4409\n",
            "step  994 / 1000 | loss 1.9618\n",
            "step  995 / 1000 | loss 2.5188\n",
            "step  996 / 1000 | loss 2.1018\n",
            "step  997 / 1000 | loss 1.7791\n",
            "step  998 / 1000 | loss 2.4764\n",
            "step  999 / 1000 | loss 2.4730\n",
            "step 1000 / 1000 | loss 2.6497\n",
            "\n",
            "--- inference (new, hallucinated names) ---\n",
            "sample  1: kamon\n",
            "sample  2: ann\n",
            "sample  3: karai\n",
            "sample  4: jaire\n",
            "sample  5: vialan\n",
            "sample  6: karia\n",
            "sample  7: yeran\n",
            "sample  8: anna\n",
            "sample  9: areli\n",
            "sample 10: kaina\n",
            "sample 11: konna\n",
            "sample 12: keylen\n",
            "sample 13: liole\n",
            "sample 14: alerin\n",
            "sample 15: earan\n",
            "sample 16: lenne\n",
            "sample 17: kana\n",
            "sample 18: lara\n",
            "sample 19: alela\n",
            "sample 20: anton\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-Pspxdk05tl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}